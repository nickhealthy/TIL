# Today I Learned

* 스웜 모드 볼륨
  * 실습 1. volume 타입의 볼륨 생성
  * 실습 2. bind 타입의 볼륨 생성
  * 스윔 모드에서 볼륨의 한계점
* 도커 스웜 모드 노드 다루기
  * Active 상태
  * Drain 상태
  * Pause 상태
  * 노드 라벨 추가
  * 서비스 제약 설정 (`node.labels`, `node.id`, `node.hostname`, `node.role`)
* Kubernetes
  * 쿠버네티스 실습 환경 설정
  * 개념
  * 실습 1
  * 실습 2

---



### 스웜 모드 볼륨

[실습페이지]([https://myanjini.tistory.com/entry/06-%EC%8A%A4%EC%9B%9C-%EB%AA%A8%EB%93%9C-%EB%B3%BC%EB%A5%A8](https://myanjini.tistory.com/entry/06-스웜-모드-볼륨))

* 도커 데몬 명령어 중 run 명령어에서 `-v` 옵션을 사용할 때는 호스트와 디렉터리를 공유하는 경우와 볼륨을 사용하는 경우에 대한 구분이 없음
* **스웜 모드에서는 서비스를 생성할 때 도커 볼륨을 사용할지, 호스트와 디렉터리를 공유할지를 명확하게 명시**

##### 준비사항

* 모든 컨테이너, 서비스, 볼륨 삭제

```bash
vagrant@swarm-manager:~$ docker service rm $(docker service ls -q)
vagrant@swarm-manager:~$ docker container rm -f $(docker container ls -aq)		⇐ 모든 노드에서 수행
vagrant@swarm-manager:~$ docker volume rm $(docker volume ls --format {{.Name}})	⇐ 모든 노드에서 수행
```



#### 실습 1. volume 타입의 볼륨 생성

1. **볼륨 생성 및 마운트**

```bash
vagrant@swarm-manager:~$ docker service create --name ubuntu --mount type=volume,source=myvol,target=/root ubuntu:14.04 ping docker.com
```

* ``--mount`` 옵션의 `type`값에 `volume`을 지정 `->` 도커 볼륨을 사용하는 서비스를 생성
* `source` `->` 사용할 볼륨 (도커 데몬에 해당 볼륨이 존재하면 해당 볼륨을 사용하고 없으면 새로 생성)
* `target` `->` 컨테이너 내부에 마운트될 디렉터리 위치

2. **`source` 옵션을 명시하지 않으면 임의의 16진수로 구성된 익명의 이름을 가진 볼륨을 생성**

```bash
docker service create --name ubuntu --mount,target=/root ubuntu:14.04 ping docker.com
```

3. **서비스의 컨테이너에서 볼륨에 공유할 컨테이너의 디렉터리에 파일이 이미 존재하면 이 파일들은 볼륨에 복사되고, 호스트에서 별도의 공간을 차지하게 됨**

![image-20200918125552530](C:\Users\Nick_주성우\AppData\Roaming\Typora\typora-user-images\image-20200918125552530.png)

4. **생성된 서비스, 컨테이너, 볼륨 삭제**

![image-20200918125626273](C:\Users\Nick_주성우\AppData\Roaming\Typora\typora-user-images\image-20200918125626273.png)

5. **서비스를 생성할 볼륨 옵션에 `volume-nocopy`를 추가하면 컨테이너의 파일들이 볼륨에 복사되지 않음**

![image-20200918125709964](C:\Users\Nick_주성우\AppData\Roaming\Typora\typora-user-images\image-20200918125709964.png)



#### 실습 2. bind 타입의 볼륨 생성

* 바인드 타입은 호스트와 디렉터리를 공유할 때 사용
* `type` 옵션의 값을 `bind`로 설정
* 볼륨 타입과 달리 공유될 호스트의 디렉터리를 설정해야 하므로 `source` 옵션을 반드시 명시해야 함

1. **호스트의 `/root/host` 디렉터리를 서비스 컨테이너의 `/root/container` 디렉터리에 마운트**

![image-20200918130018126](C:\Users\Nick_주성우\AppData\Roaming\Typora\typora-user-images\image-20200918130018126.png)

#### 스윔 모드에서 볼륨의 한계점

* 서비스를 할당받을 수 있는 모든 노드가 볼륨 데이터를 가지고 있어야 하므로 스웜 클러스터에서 볼륨을 사용
  * 로컬 볼륨이 존재하지 않는 노드에는 컨테이너가 할당될 수 없음
* 일반적인 해법은 어느 노드에서도 접근 가능한 **퍼시스턴트 스토리지(Persistent Storage)**를 사용하는 것
* 퍼시스턴트 스토리지
  * 호스트와 컨테이너와 별개로 외부에 존재해 네트워크로 마운트할 수 있는 스토리지
  * 도커 자체가 제공하지 않으므로 서드파티 플로그인을 사용하거나 nfs, dfs 등을 별도로 구성해야 함

---



### 도커 스웜 모드 노드 다루기

* 마스터 노드는 최대한 부하를 받지 않도록 서비스를 할당받지 않게 하거나, 문제가 발생한 특정 노드를 유지보수할 때 해당 노드에 컨테이너를 할당하지 않게 만들고 싶을 때 등 `docker node update --availability` 명령으로 **노드의 상태를 변경할 수 있음**
* [실습페이지]([https://myanjini.tistory.com/entry/07-%EB%8F%84%EC%BB%A4-%EC%8A%A4%EC%9B%9C-%EB%AA%A8%EB%93%9C-%EB%85%B8%EB%93%9C-%EB%8B%A4%EB%A3%A8%EA%B8%B0](https://myanjini.tistory.com/entry/07-도커-스웜-모드-노드-다루기))



![도커 스웜 모드 노드 다루기 1](C:\dev\TIL\2020_09_18\img\도커 스웜 모드 노드 다루기 1.PNG)



![도커 스웜 모드 노드 다루기 2](C:\dev\TIL\2020_09_18\img\도커 스웜 모드 노드 다루기 2.PNG)



![도커 스웜 모드 노드 다루기 3](C:\dev\TIL\2020_09_18\img\도커 스웜 모드 노드 다루기 3.PNG)



![도커 스웜 모드 노드 다루기 4](C:\dev\TIL\2020_09_18\img\도커 스웜 모드 노드 다루기 4.PNG)



![도커 스웜 모드 노드 다루기 5](C:\dev\TIL\2020_09_18\img\도커 스웜 모드 노드 다루기 5.PNG)



![도커 스웜 모드 노드 다루기 6](C:\dev\TIL\2020_09_18\img\도커 스웜 모드 노드 다루기 6.PNG)

#### 실습

1. **노드의 상태 확인**

```bash
vagrant@swarm-manager:~$ docker node ls

ID                            HOSTNAME            STATUS              AVAILABILITY     
repfyu4iazgigfjed6hvvz63i     swarm-worker2       Ready               `Active`                                  19.03.6

```

2. **swarm-worker2 노드를 `Drain` 상태로 변경**

```bash
vagrant@swarm-manager:~$ docker node update --availability drain swarm-worker2
swarm-worker2

# swarm-worker2 노드가 Drain 상태인 것을 확인
vagrant@swarm-manager:~$ docker node ls

`repfyu4iazgigfjed6hvvz63i     swarm-worker2       Ready               Drain          
```

3. **nginx 서비스를 생성 (5개의 컨테이너를 기동)**

```bash
vagrant@swarm-manager:~$ docker service create --name nginx --replicas 5 nginx
```

4. **서비스 생성 여부 확인**

```bash
vagrant@swarm-manager:~$ docker service ls
```

5. **서비스 실행 노드를 확인 (worker2는 빠져가 있음)**

```bash
vagrant@swarm-manager:~$ docker service ps nginx
```

6.  **swarm-worker2 노드를 Active 상태로 전환**

```bash
vagrant@swarm-manager:~$ docker node update --availability active swarm-worker2
`swarm-worker2
```

7. **swarm-worker2 노드가 Active된 것을 확인**

```bash
vagrant@swarm-manager:~$ docker node ls
```

8. **swarm-worker2 노드가 활성화되어도 nignx 서비스가 re-balance(재할당, 재배치)되지 않은 것을 확인**
   * 자동으로 로드 밸런싱을 해주진 않음
   * **다시 스케일을 줄였다가 늘려서 로드 밸런싱을 맞춰줘야함**

```bash
vagrant@swarm-manager:~$ docker service ps nginx
```

9. **스케일 다운**

```bash
vagrant@swarm-manager:~$ docker service scale nginx=1
```

10. **남아있는 컨테이너를 확인**

```bash
vagrant@swarm-manager:~$ docker service ps nginx
```

11. **스케일 업**

```bash
vagrant@swarm-manager:~$ docker service scale nginx=5
```

12. **Active 상태인 노드에 컨테이너가 골고루 배치된 것을 확인**

```bash
vagrant@swarm-manager:~$ docker service ps nginx
```

---



### Kubernets [참고](https://myanjini.tistory.com/entry/o2-Kubernetes)

* 오케스트레이션 : 컨테이너를 관리하는 도구
* 도커 스웜 모드에서 컨테이너를 관리하는 것을 보다 편리하게 관리가 가능

<img src="C:\dev\TIL\2020_09_18\img\쿠버네티스 1.PNG" alt="쿠버네티스 1" style="zoom:50%;" />

<img src="C:\dev\TIL\2020_09_18\img\쿠버네티스 2.PNG" alt="쿠버네티스 2" style="zoom:50%;" />

<img src="C:\dev\TIL\2020_09_18\img\쿠버네티스 3.PNG" alt="쿠버네티스 3" style="zoom:50%;" />

<img src="C:\dev\TIL\2020_09_18\img\쿠버네티스 4.PNG" alt="쿠버네티스 4" style="zoom: 50%;" />



#### 쿠버네티스 실습 환경 설정

1. **Vagrantfile 가상머신 구축 (ubuntu이용)**

```Vagrantfile
C:\kubernetes\Vagrantfile

# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/bionic64"
  config.vm.hostname = "ubuntu"
  config.vm.network "private_network", ip: "192.168.111.110"
  config.vm.synced_folder ".", "/home/vagrant/sync", disabled: true
  config.vm.provider "virtualbox" do |vb|
    vb.cpus = 2
    vb.memory = 2048
  end
end

C:\kubernetes> vagrant up
C:\kubernetes> vagrant ssh
```

2. **패키지 최신화**

```bash
vagrant@ubuntu:~$ sudo su
root@ubuntu:/home/vagrant# cd
root@ubuntu:~# apt update
root@ubuntu:~# apt upgrade
```

3. **도커 설치 및 설정**

```bash
root@ubuntu:~# apt install docker.io -y
root@ubuntu:~# usermod -a -G docker vagrant
root@ubuntu:~# service docker restart
root@ubuntu:~# chmod 666 /var/run/docker.sock
```

4. **쿠버네티스 설치** [**kubectl 설치**](https://kubernetes.io/ko/docs/tasks/tools/install-kubectl/)

```bash
# 루트로 들어가서 설치
sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
# 레파지토리 정보 추가
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubectl

# 버전 확인
root@ubuntu:~# kubectl version
# 스웜(클러스터)를 아직 설정 안해줘서 나는 에러임
`The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

5. **[Minikube 설치](https://kubernetes.io/ko/docs/tasks/tools/install-minikube/)** 

```bash
# 루트로 들어가서 설치
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \
  && chmod +x minikube
sudo mkdir -p /usr/local/bin/
sudo install minikube /usr/local/bin/
```

6. **클러스터 시작**

```bash
vagrant@ubuntu:~$ minikube start
vagrant@ubuntu:~$ minikube status
중지 ⇒ minikube stop
삭제 ⇒ minikube delete
# 설치된 후 확인
# 정상적으로 연결된 것
vagrant@ubuntu:~$ kubectl version
vagrant@ubuntu:~$ kubectl version --short
```

---



#### 개념

* **포드(pod)**
  * **컨테이너 애플리케이션**의 기본 단위
  * 1개 이상의 컨테이너로 구성된 컨테이너의 집합
  * 여러 개의 컨테이너를 추상화해서 하나의 애플리케이션으로 동작하도록 묶어 놓은 컨테이너의 묶음

#### 실습 1

1. **nginx 컨테이너로 구성된 포드를 생성** (두가지 방법 : docker swarm create = apply / run = run)

```yaml
vagrant@ubuntu:~/kub01$ vi nginx-pod.yml


apiVersion: v1                           ⇐ YAML 파일에서 정의한 오브젝트의 API 버전
kind: Pod                                ⇐ 리소스의 종류 (kubectl api-resources 명령의 KIND 항목)
metadata:                                ⇐ 라벨, 주석, 이름과 같은 리소스의 부가 정보
  name: my-nginx-pod
spec:
  containers:                            ⇐ 리소스 생성을 위한 정보
  - name: my-nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP
```

2. **새로운 파드 생성 및 확인**

```bash
vagrant@ubuntu:~/kub01$ kubectl apply -f nginx-pod.yml
`pod/my-nginx-pod created

vagrant@ubuntu:~/kub01$ kubectl get pods
vagrant@ubuntu:~/kub01$ kubectl get po			⇐ pods의 축약어(po)를 이용할 수 있음
```

3. **생성된 리소스의 자세한 정보를 확인**

```bash
vagrant@ubuntu:~/kub01$ kubectl describe pods my-nginx-pod

`IP:           172.18.0.3
`IPs:
`  IP:  172.18.0.3
Containers:
```

4. **클러스터 내부에 테스트를 위한 임시 포드를 생성해서 nginx 포드의 동작을 확인**

```bash
# alicek106/ubuntu:curl 이미지를 이용해서 debug 이름의 포드를 생성
vagrant@ubuntu:~/kub01$ kubectl run -it --rm debug --image=alicek106/ubuntu:curl --restart=Never bash

# debug 포드가 생성(실행)된 상태에서 포드를 조회
vagrant@ubuntu:~$ kubectl get pods
`NAME           READY   STATUS    RESTARTS   AGE
`debug          1/1     Running   0          58s
`my-nginx-pod   1/1     Running   0          8m13s

# debug 포드에서 my-nginx-pod(172.18.0.3)로 요청을 전달
root@debug:/# curl 172.18.0.3

# debug 포드를 빠져 나오는 것과 동시에 삭제되는 것을 확인
root@debug:/# exit
`pod "debug" deleted

vagrant@ubuntu:~/kub01$ kubectl get pods
`NAME           READY   STATUS    RESTARTS   AGE
`my-nginx-pod   1/1     Running   0          12m
```

5. **kubectl exec 명령으로 포드의 컨테이너에 명령어를 전달**

```bash
vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod -- bash
`root@my-nginx-pod:/# ls /etc/nginx/
```

6. **kubectl logs 명령으로 포드의 로그를 확인**

```bash
vagrant@ubuntu:~/kub01$ kubectl logs my-nginx-pod
```

7. **쿠버네티스 오브젝트를 삭제**

```bash
vagrant@ubuntu:~/kub01$ kubectl delete -f nginx-pod.yml
vagrant@ubuntu:~/kub01$ kubectl get pods
```

---



#### 실습 2

1. **2개의 컨테이너로 구성된 pod 정의**

```yaml
vagrant@ubuntu:~/kub01$ vi nginx-pod-with-ubuntu.yml

apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod
spec:
  containers:
  - name: my-nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP

  - name: ubuntu-sidecar-container
    image: alicek106/rr-test:curl
    command: [ "tail" ]
    args: [ "-f", "/dev/null" ]
```

2. **pod 생성**

```bash
vagrant@ubuntu:~/kub01$ kubectl apply -f nginx-pod-with-ubuntu.yml
`pod/my-nginx-pod created

vagrant@ubuntu:~/kub01$ kubectl get pods
`my-nginx-pod   2/2     Running   0          21s
`my-nginx-pod에는 my-nginx-container 컨테이너와 ubuntu-sidecar-container 컨테이너가 실행 중

my-nginx-pod에는 my-nginx-container 컨테이너와 ubuntu-sidecar-container 컨테이너가 실행 중

# pod로 명령어를 전달할 때 실행할 컨테이너를 지정하면 default로 설정된 컨테이너가 실행
vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod -- bash

# 특정 컨테이너에게 명령어를 전달할 때는 -c 옵션을 사용
vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod -c `ubuntu-sidecar-container` -- bash

# 우분투 컨테이너 내부에서 localhost 요청에 대해 응답이 도착하는 것을 확인
# 우분투 컨테이너의 localhost에서 nginx 서버로 접근이 가능
# 포드 내부의 컨테이너들은 네트워크와 같은 리눅스 네임스페이스를 공유
root@my-nginx-pod:/# curl localhost
```

3. **ubunut-sidecar-container 컨테이너만으로 구성된 pod를 생성한 후 해당 pod에서 curl을 동작**

```bash
vagrant@ubuntu:~/kub01$ cp nginx-pod-with-ubuntu.yml nginx-pod-test.yml
vagrant@ubuntu:~/kub01$ vi nginx-pod-test.yml

apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod-test
spec:
  containers:
  - name: ubuntu-sidecar-container
    image: alicek106/rr-test:curl           `⇐ 웹 서버를 포함하고 있지 않음
    command: [ "tail" ]
    args: [ "-f", "/dev/null" ]
    
vagrant@ubuntu:~/kub01$ kubectl apply -f nginx-pod-test.yml
`pod/my-nginx-pod-test created

vagrant@ubuntu:~/kub01$ kubectl exec -it my-nginx-pod-test -- bash
root@my-nginx-pod-test:/# curl localhost
`curl: (7) Failed to connect to localhost port 80: Connection refused	⇐ 80 포트로 서비스를 제공하고 있지 않다
```