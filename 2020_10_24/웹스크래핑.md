# Today I Learned

* HTML 기본
  * xpath
* requests
  * requests 모듈 인식 오류
  * requests 기본
* regular expression
* User Agent
* BS4(BeautifulSoup)
* 네이버 웹툰 연습
* HTTP METHOD
* 쿠팡 노트북 스크래핑

---



### HTML 기본

태그(element), 속성, 태그 값

#### xpath

* HTML에서 비슷한 Tag 혹은 element가 **정확히 어떤 것을 지칭하는지 알기 위해서 사용 (경로)**
  * 특징이나 속성들을 이용해 경로를 줄일 수 있음

```javascript
# 전체 경로
/학교/학년/반/학생[2]
# 특정한 값으로 이용할 수도 있음
//* [@학번="1-1-5"]

/html/body/div/span/a...
//*[@id="login"]
```

* `/` : 지금 내가 위치한 곳에서 한단계 아래를 표시
* `//` : 지금 내가 위치한 곳에서 모든 하위 엘리먼트들에서 다 찾음
* `*` : 모든 것을 찾음

---



### requests

#### requests 모듈 인식 오류

python은 모듈 이름과 소스코드 파일 이름을 구별하지 못함

* 같은 이름을 쓰면 안됨
  * ex) `requests.py`

#### requests 기본

* requests.get(*URL주소*) 코드로 해당 URL 주소를 요청함
* *requests.raise_for_status()* : 200 상태 코드가 떨어지면 넘어가고, URL 상태코드가 다른 것이 떨어지면 강제에러 발생

---



### regular expression

* ` .` (ca.e) : 하나의 문자를 의미 > care, cafe, case (o) | caffe (x)
* `^` (^de) : 문자열의 시작 > desk, destination (o) | fade (x)
* `$` (se$) : 문자열의 끝 > case, base (o)  | face (x)

```python
import re

p = re.compile("ca.e") # 이것을 검열할 것이다.
# . (ca.e) : 하나의 문자를 의미 > care, cafe, case (o) | caffe (x)
# ^ (^de) : 문자열의 시작 > desk, destination (o) | fade (x)
# $ (se$) : 문자열의 끝 > case, base (o)  | face (x)


def print_match(m):
    if m:
        # 일치하는 문자열 반환
        print("m.group():", m.group())
        # 일치하는 문자열이 있을 시 입력받은 문자열 그대로 출력
        print("m.string:", m.string)
        # 일치하는 문자열의 시작 인덱스
        print("m.start():", m.start())
        # 일치하는 문자열의 끝 인덱스
        print("m.end():", m.end())
        # 일치하는 문자열의 시작과 끝 인덱스
        print("m.span():", m.span())
    else:
        print("매칭되지 않음")

# match : 주어진 문자열의 처음부터 일치하는지 확인
# m = p.match("lesscare") 
# print_match(m)

# search : 주어진 문열열 중에 매칭되는 것이 있는지 확인
# m = p.search("good care") 
# print_match(m)

# findall : 일치하는 모든 것을 리스트 형태로 제공
lst = p.findall("good care cafe")
print(lst)


# summary

# 1. p = re.compile("원하는 정규식 형태")
# 2. m = p.match("비교할 문자열") : 주어진 문자열의 처음부터 일치하는지 확인
# 2-1. m = p.search("비교할 문자열") : 주어진 문자열 중에 일치하는게 있는지 확인
# 2-2. lst = p.findall("비교할 문자열") : 일치하는 모든 것을 리스트 형태로 제공

# 원하는 형태 : 정규식
# . (ca.e) : 하나의 문자를 의미 > care, cafe, case (o) | caffe (x)
# ^ (^de) : 문자열의 시작 > desk, destination (o) | fade (x)
# $ (se$) : 문자열의 끝 > case, base (o)  | face (x)
```

---



### User Agent

* 브라우저는 사용자 정보를 헤더를 통해 알 수 있음
  * 브라우저에서 접속하는 헤더 정보에 따라 각각의 맞는 값을 제공
  * 어떤 매개체로 접속했는지 등(웹 or 모바일 접속)
* 브라우저는 사용자가 아닌 스크래핑 등으로 접속한 것을 막음
  * 서버 부하 등의 이유로
  * 해결 방법은 **User Agent를 사용**
    * *user agent string* 검색해 나의 유저 정보를 확인

* 사용 방법

```python
url = 'http://nadocoding.tistory.com'
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36"
}
res = requests.get(url, headers=headers)
```

---



### BS4(BeautifulSoup)

필요한 서드파티 모듈 설치

```python
pip install beautifulsoup

# parsing 작업을 위해 필요
pip install lxml
```



Beautifulsoup 연습

```python
import requests
from bs4 import BeautifulSoup

url = "https://comic.naver.com/index.nhn"
res = requests.get(url)
res.raise_for_status()

# html 문서를 lmxl parser를 통해서 beautifulSoup 객체로 만듬
soup = BeautifulSoup(res.text, "lxml")

# .title : title 태그를 포함한 제일 첫번째 값을 가져옴
# print(soup.title)

# get_text() : html 태그를 제외한 텍스트만 가져오기
# print(soup.title.get_text())

# a.attrs : a 태그의 '속성명', '속성값'을 dict 형태로 가져옴
# print(soup.a.attrs)

# a["href"]의 href 속성 '값' 정보를 출력
# print(soup.a["href"])

# h2 태그의 속성명이 'class'이고, 속성 값은 'Ntxt_today_webtoon'인 태그를 가져와라
# print(soup.find("h2", attrs={"class":"Ntxt_today_webtoon"}))
# 속성명이 'class'이고, 속성 값은 'Ntxt_today_webtoon'인 어떤 태그를 가져와줘
# print(soup.find(attrs={"class":"Ntxt_today_webtoon"}))

# li 태그에서  "class":"rank01"인 첫번째 값을 가져와줘
# print(soup.find("li", attrs={"class":"rank01"}))
# rank1 = soup.find("li", attrs={"class":"rank01"})
# rank1에서 a 태그만 가져와줘
# print(rank1.a.get_text())
```



태그들의 부모, 형제 태그 가져오기 (하나씩)

```python
rank1 = soup.find('li', attrs={'class':"rank01"})
# next_sibling : 형제간의 다음 태그를 지칭
rank2 = rank1.next_sibling.next_sibling
rank3 = rank2.next_sibling.next_sibling
# print(rank3.a.get_text())

# previous_sibling : 형제간의 태그를 지칭
# rank2 = rank3.previous_sibling.previous_sibling
# print(rank2.a.get_text())
# print(rank1.parent)

# 개행 문자로 인해 next_sibling를 두번씩 써준 거 상관없이, 태그를 기준으로 가져오기
# rank2 = rank1.find_next_sibling("li")
# print(rank2.a.get_text())
# rank3 = rank2.find_next_sibling("li")
# print(rank3.a.get_text())
# rank2 = rank3.find_previous_sibling("li")
# print(rank2.a.get_text())
```



태그들의 부모, 형제 태그 가져오기 (한번에)

```python
# find_next_siblings 태그 값 이후의 값들을 모두 가져온다.
print(rank1.find_next_siblings("li"))
print(rank1.a.get_text())
```

---



### 네이버 웹툰 연습

```python
import requests
from bs4 import BeautifulSoup


url = "https://comic.naver.com/webtoon/list.nhn?titleId=675554"
res = requests.get(url)
res.raise_for_status()

soup = BeautifulSoup(res.text, "lxml")
# cartoons = soup.find_all("td", attrs={"class":"title"})

# title = cartoons[0].a.get_text()
# link = cartoons[0].a['href']
# print(title, "https://comic.naver.com" + link)

# 만화 제목 + 링크 구하기
# for cartoon in cartoons:
#     title = cartoon.a.get_text()
#     link = "https://comic.naver.com" + cartoon.a['href']
#     print(title, link)

# 평점 구하기
total_rates = 0
cartoons = soup.find_all("div", attrs={"class":"rating_type"})
for cartoon in cartoons:
    rate = cartoon.find('strong').get_text()
    total_rates += float(rate)
print("전체 점수 :", round(total_rates, 2))
print("평균 점수 :", round(total_rates / len(cartoons),2))
```

---



### HTTP METHOD

* GET
  * URL에 정보를 적어서 보여줌
* POST
  * HTTP 메시지 바디에 숨겨서 보여줌

---



### 쿠팡 노트북 스크래핑 (한페이지)

리뷰 30개 이상, 평점 4.5 이상 상품 가져오기

* 광고 상품 제외
* 애플 상품 제외
* 평점, 평점 수 없는 제품 제외

```python
import requests
from bs4 import BeautifulSoup
import re

url = "https://www.coupang.com/np/search?q=%EB%85%B8%ED%8A%B8%EB%B6%81&channel=user&component=&eventCategory=SRP&trcid=&traid=&sorter=scoreDesc&minPrice=&maxPrice=&priceRange=&filterType=&listSize=36&filter=&isPriceRange=false&brand=&offerCondition=&rating=0&page=1&rocketAll=false&searchIndexingToken=&backgroundColor="
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'
}

res = requests.get(url,headers=headers)
res.raise_for_status()

soup = BeautifulSoup(res.text, "lxml")


items = soup.find_all("li", attrs={"class":re.compile("^search-product")})
print(items[0].find("div", attrs={"class":"name"}).get_text())

for item in items:

    # 광고 상품 제외
    ad_badge = item.find("span", attrs={"class":"ad-badge-text"})
    if ad_badge:
        print("  <광고 상품 제외합니다>")
        continue

    # 리뷰 30개 이상, 평점 4.5 이상
    name = item.find("div", attrs={"class":"name"}).get_text()
    # 애플 상품 제외
    if "Apple" in name:
        print("  <Apple 상품 제외 합니다>")
        continue

    price = item.find("strong", attrs={"class":"price-value"})
    if price:
        price = price.get_text()
    else:
        price = "가격 없음"

    rate = item.find("em", attrs={"class":"rating"})
    if rate:
        rate = rate.get_text() 
    else:
        print(" <평점 없는 상품 제외>")
        continue
    
    rate_count = item.find("span", attrs={"class":"rating-total-count"})
    if rate_count:
        rate_count = rate_count.get_text() # (30)
        rate_count = rate_count[1:-1]
    else:
        print("평점 수 없는 상품 제외>")
        continue
    if float(rate) >= 4.5 and int(rate_count) >= 30:
        print(name, price+"만원", rate, rate_count)
```

---



### 쿠팡 노트북 스크래핑 (다중 페이지)

```python
import requests
from bs4 import BeautifulSoup
import re


headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'
}

for i in range(1, 6):
    print("페이지 : ", i)
    url = "https://www.coupang.com/np/search?q=%EB%85%B8%ED%8A%B8%EB%B6%81&channel=user&component=&eventCategory=SRP&trcid=&traid=&sorter=scoreDesc&minPrice=&maxPrice=&priceRange=&filterType=&listSize=36&filter=&isPriceRange=false&brand=&offerCondition=&rating=0&page={}&rocketAll=false&searchIndexingToken=&backgroundColor=".format(i)
    res = requests.get(url,headers=headers)
    res.raise_for_status()

    soup = BeautifulSoup(res.text, "lxml")

    items = soup.find_all("li", attrs={"class":re.compile("^search-product")})

    for item in items:
        
        # 광고 상품 제외
        ad_badge = item.find("span", attrs={"class":"ad-badge-text"})
        if ad_badge:
        #    print("  <광고 상품 제외합니다>")
            continue

        # 리뷰 30개 이상, 평점 4.5 이상
        name = item.find("div", attrs={"class":"name"}).get_text()
        # 애플 상품 제외
        if "Apple" in name:
            # print("  <Apple 상품 제외 합니다>")
            continue

        price = item.find("strong", attrs={"class":"price-value"})
        if price:
            price = price.get_text()
        else:
            continue

        rate = item.find("em", attrs={"class":"rating"})
        if rate:
            rate = rate.get_text() 
        else:
        #    print(" <평점 없는 상품 제외>")
            continue
        
        rate_count = item.find("span", attrs={"class":"rating-total-count"})
        if rate_count:
            rate_count = rate_count.get_text()[1:-1] # (30)
        else:
            # print("평점 수 없는 상품 제외>")
            continue

        link = item.find("a", attrs={"class":"search-product-link"})['href']
        
        if float(rate) >= 4.5 and int(rate_count) >= 30:
            # print(name, price+"만원", rate, rate_count)
            print(f"제품명 : {name}")
            print(f"가격 : {price}")
            print(f"평점 : {rate}점 ({rate_count}개)")
            print("바로가기 : {}".format("https://www.coupang.com" + link))
            print("-"*100)
```

---



